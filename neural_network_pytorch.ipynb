{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"diabetes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Missing Values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicated value Present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent and Independent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "X=df.drop(columns=['Outcome']).values\n",
    "y=df['Outcome'].values\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 8)\n",
      "(154, 8)\n",
      "(614,)\n",
      "(154,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting numpy array to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=torch.from_numpy(X_train_scaled.astype(np.float32))\n",
    "X_test_tensor=torch.from_numpy(X_test_scaled.astype(np.float32))\n",
    "y_train_tensor=torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test_tensor=torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5166,  0.7505,  0.5648,  ...,  0.7958,  0.5295,  0.5679],\n",
       "        [ 1.8120,  0.2448, -0.3479,  ...,  1.2287, -0.0697,  0.3984],\n",
       "        [ 0.9257, -0.6087,  0.2605,  ...,  0.7040, -0.7942,  0.9916],\n",
       "        ...,\n",
       "        [ 2.6983,  0.1499,  1.0718,  ...,  1.5172,  0.3644,  0.7374],\n",
       "        [ 0.0395,  1.5724,  0.1591,  ...,  1.5434,  0.0465, -0.6184],\n",
       "        [ 1.5166, -0.6087,  0.3619,  ...,  0.1400,  0.6151,  1.0764]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic Model Building using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \"\"\"Some Information about MyModule\"\"\"\n",
    "    def __init__(self,input_size):\n",
    "        super().__init__()\n",
    "        self.first_layer=nn.Linear(input_size,1,dtype=torch.float32)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out=self.first_layer(features)\n",
    "        out=self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MyNet(X_train_tensor.shape[1])\n",
    "# y_pred=model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 Loss: 0.7385072708129883\n",
      "epoch: 2 Loss: 0.6715041995048523\n",
      "epoch: 3 Loss: 0.6214901804924011\n",
      "epoch: 4 Loss: 0.5843688249588013\n",
      "epoch: 5 Loss: 0.5564995408058167\n",
      "epoch: 6 Loss: 0.5348109006881714\n",
      "epoch: 7 Loss: 0.5175073742866516\n",
      "epoch: 8 Loss: 0.5040386915206909\n",
      "epoch: 9 Loss: 0.494070827960968\n",
      "epoch: 10 Loss: 0.48720577359199524\n",
      "epoch: 11 Loss: 0.48293250799179077\n",
      "epoch: 12 Loss: 0.48056039214134216\n",
      "epoch: 13 Loss: 0.47925978899002075\n",
      "epoch: 14 Loss: 0.47837984561920166\n",
      "epoch: 15 Loss: 0.4776502549648285\n",
      "epoch: 16 Loss: 0.4770512878894806\n",
      "epoch: 17 Loss: 0.47662854194641113\n",
      "epoch: 18 Loss: 0.47640708088874817\n",
      "epoch: 19 Loss: 0.47639262676239014\n",
      "epoch: 20 Loss: 0.4765886962413788\n",
      "epoch: 21 Loss: 0.47698283195495605\n",
      "epoch: 22 Loss: 0.47751301527023315\n",
      "epoch: 23 Loss: 0.47806182503700256\n",
      "epoch: 24 Loss: 0.47849714756011963\n",
      "epoch: 25 Loss: 0.47872427105903625\n",
      "epoch: 26 Loss: 0.4787120521068573\n",
      "epoch: 27 Loss: 0.4784848988056183\n",
      "epoch: 28 Loss: 0.4780963957309723\n",
      "epoch: 29 Loss: 0.47760406136512756\n",
      "epoch: 30 Loss: 0.4770524799823761\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    y_pred=model(X_train_tensor)\n",
    "    loss=loss_function(y_pred,y_train_tensor.view(-1,1))\n",
    "    print(f\"epoch: {i+1} Loss: {loss}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4725,  1.4507, -0.2517,  0.0231, -0.2011,  0.6583,  0.2322,  0.0678]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.first_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2397,  0.1899, -0.0900,  0.2251,  0.1139,  0.1430, -0.0517,  0.2978]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.first_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Model using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLModel(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(num_features,15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15,7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(7,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "    def forward(self,features):\n",
    "        out=self.network(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel=MyMLModel(X_train_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer=torch.optim.AdamW(newmodel.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6710340976715088, Val Loss: 0.6723341345787048, Val Accuracy: 0.3571\n",
      "Epoch: 2, Loss: 0.6638672947883606, Val Loss: 0.6681098341941833, Val Accuracy: 0.3571\n",
      "Epoch: 3, Loss: 0.6544120907783508, Val Loss: 0.6580721139907837, Val Accuracy: 0.3571\n",
      "Epoch: 4, Loss: 0.6507963538169861, Val Loss: 0.6550838947296143, Val Accuracy: 0.3571\n",
      "Epoch: 5, Loss: 0.6482145190238953, Val Loss: 0.6531212329864502, Val Accuracy: 0.3571\n",
      "Epoch: 6, Loss: 0.6465647220611572, Val Loss: 0.6520583629608154, Val Accuracy: 0.3571\n",
      "Epoch: 7, Loss: 0.6457180380821228, Val Loss: 0.6517570614814758, Val Accuracy: 0.3571\n",
      "Epoch: 8, Loss: 0.6455132961273193, Val Loss: 0.6520459055900574, Val Accuracy: 0.3571\n",
      "Epoch: 9, Loss: 0.6457650661468506, Val Loss: 0.6527288556098938, Val Accuracy: 0.3571\n",
      "Epoch: 10, Loss: 0.6462785005569458, Val Loss: 0.6536023020744324, Val Accuracy: 0.3571\n",
      "Epoch: 11, Loss: 0.6468722224235535, Val Loss: 0.6544783115386963, Val Accuracy: 0.3571\n",
      "Epoch: 12, Loss: 0.6474003195762634, Val Loss: 0.6552086472511292, Val Accuracy: 0.3571\n",
      "Epoch: 13, Loss: 0.6477653384208679, Val Loss: 0.655697762966156, Val Accuracy: 0.3571\n",
      "Epoch: 14, Loss: 0.6479228138923645, Val Loss: 0.6559054851531982, Val Accuracy: 0.3571\n",
      "Epoch: 15, Loss: 0.6478724479675293, Val Loss: 0.6558390855789185, Val Accuracy: 0.3571\n",
      "Epoch: 16, Loss: 0.6476466059684753, Val Loss: 0.6555398106575012, Val Accuracy: 0.3571\n",
      "Epoch: 17, Loss: 0.6472971439361572, Val Loss: 0.6550693511962891, Val Accuracy: 0.3571\n",
      "Epoch: 18, Loss: 0.6468708515167236, Val Loss: 0.6544968485832214, Val Accuracy: 0.3571\n",
      "Epoch: 19, Loss: 0.6463780403137207, Val Loss: 0.6538380980491638, Val Accuracy: 0.3571\n",
      "Epoch: 20, Loss: 0.645795464515686, Val Loss: 0.6528937220573425, Val Accuracy: 0.3571\n",
      "Epoch: 21, Loss: 0.6447874903678894, Val Loss: 0.6511498689651489, Val Accuracy: 0.3571\n",
      "Epoch: 22, Loss: 0.6430479884147644, Val Loss: 0.6484567523002625, Val Accuracy: 0.3571\n",
      "Epoch: 23, Loss: 0.6405467987060547, Val Loss: 0.6458321809768677, Val Accuracy: 0.3571\n",
      "Epoch: 24, Loss: 0.6377131938934326, Val Loss: 0.641994059085846, Val Accuracy: 0.3571\n",
      "Epoch: 25, Loss: 0.6348226070404053, Val Loss: 0.6372318267822266, Val Accuracy: 0.3571\n",
      "Epoch: 26, Loss: 0.6312331557273865, Val Loss: 0.6313158869743347, Val Accuracy: 0.3571\n",
      "Epoch: 27, Loss: 0.6269655227661133, Val Loss: 0.6259469389915466, Val Accuracy: 0.3571\n",
      "Epoch: 28, Loss: 0.621874213218689, Val Loss: 0.6190316081047058, Val Accuracy: 0.3571\n",
      "Epoch: 29, Loss: 0.6162129044532776, Val Loss: 0.6115861535072327, Val Accuracy: 0.3571\n",
      "Epoch: 30, Loss: 0.6091787815093994, Val Loss: 0.6016805171966553, Val Accuracy: 0.3571\n",
      "Epoch: 31, Loss: 0.6013264656066895, Val Loss: 0.5905977487564087, Val Accuracy: 0.3571\n",
      "Epoch: 32, Loss: 0.593608558177948, Val Loss: 0.578533411026001, Val Accuracy: 0.3571\n",
      "Epoch: 33, Loss: 0.5854129791259766, Val Loss: 0.569101870059967, Val Accuracy: 0.3571\n",
      "Epoch: 34, Loss: 0.5785060524940491, Val Loss: 0.5648787617683411, Val Accuracy: 0.3571\n",
      "Epoch: 35, Loss: 0.5708110332489014, Val Loss: 0.5563491582870483, Val Accuracy: 0.3571\n",
      "Epoch: 36, Loss: 0.5627455711364746, Val Loss: 0.5479691028594971, Val Accuracy: 0.3571\n",
      "Epoch: 37, Loss: 0.5577787160873413, Val Loss: 0.542422354221344, Val Accuracy: 0.7792\n",
      "Epoch: 38, Loss: 0.5511177778244019, Val Loss: 0.5382049679756165, Val Accuracy: 0.7792\n",
      "Epoch: 39, Loss: 0.5440271496772766, Val Loss: 0.5313557386398315, Val Accuracy: 0.7987\n",
      "Epoch: 40, Loss: 0.5403025150299072, Val Loss: 0.5275802612304688, Val Accuracy: 0.7987\n",
      "Epoch: 41, Loss: 0.5361745357513428, Val Loss: 0.5338718295097351, Val Accuracy: 0.7792\n",
      "Epoch: 42, Loss: 0.5312845706939697, Val Loss: 0.542202353477478, Val Accuracy: 0.7922\n",
      "Epoch: 43, Loss: 0.5267770290374756, Val Loss: 0.5427393913269043, Val Accuracy: 0.7987\n",
      "Epoch: 44, Loss: 0.5252333879470825, Val Loss: 0.5363218188285828, Val Accuracy: 0.7987\n",
      "Epoch: 45, Loss: 0.5204917788505554, Val Loss: 0.5340467095375061, Val Accuracy: 0.7987\n",
      "Epoch: 46, Loss: 0.5147409439086914, Val Loss: 0.5322535037994385, Val Accuracy: 0.7857\n",
      "Epoch: 47, Loss: 0.5122509598731995, Val Loss: 0.5307981967926025, Val Accuracy: 0.7857\n",
      "Epoch: 48, Loss: 0.5085576176643372, Val Loss: 0.5310346484184265, Val Accuracy: 0.7792\n",
      "Epoch: 49, Loss: 0.5051289200782776, Val Loss: 0.5324328541755676, Val Accuracy: 0.7857\n",
      "Epoch: 50, Loss: 0.5036917924880981, Val Loss: 0.532426655292511, Val Accuracy: 0.7857\n",
      "Epoch: 51, Loss: 0.5022515654563904, Val Loss: 0.5308119058609009, Val Accuracy: 0.7792\n",
      "Epoch: 52, Loss: 0.5005019903182983, Val Loss: 0.530258059501648, Val Accuracy: 0.7792\n",
      "Epoch: 53, Loss: 0.49926286935806274, Val Loss: 0.5301859974861145, Val Accuracy: 0.7727\n",
      "Epoch: 54, Loss: 0.49741873145103455, Val Loss: 0.5310720801353455, Val Accuracy: 0.7792\n",
      "Epoch: 55, Loss: 0.495387464761734, Val Loss: 0.5324243307113647, Val Accuracy: 0.7857\n",
      "Epoch: 56, Loss: 0.49252617359161377, Val Loss: 0.5334035754203796, Val Accuracy: 0.7792\n",
      "Epoch: 57, Loss: 0.49091899394989014, Val Loss: 0.5309932827949524, Val Accuracy: 0.7857\n",
      "Epoch: 58, Loss: 0.48885616660118103, Val Loss: 0.5253129601478577, Val Accuracy: 0.7792\n",
      "Epoch: 59, Loss: 0.48697957396507263, Val Loss: 0.5189348459243774, Val Accuracy: 0.7857\n",
      "Epoch: 60, Loss: 0.4849420189857483, Val Loss: 0.5150255560874939, Val Accuracy: 0.7857\n",
      "Epoch: 61, Loss: 0.4829423725605011, Val Loss: 0.5148330330848694, Val Accuracy: 0.7857\n",
      "Epoch: 62, Loss: 0.4816899001598358, Val Loss: 0.5191384553909302, Val Accuracy: 0.7792\n",
      "Epoch: 63, Loss: 0.48003411293029785, Val Loss: 0.5232686996459961, Val Accuracy: 0.7727\n",
      "Epoch: 64, Loss: 0.47771933674812317, Val Loss: 0.5220995545387268, Val Accuracy: 0.7727\n",
      "Epoch: 65, Loss: 0.4761257469654083, Val Loss: 0.5206491947174072, Val Accuracy: 0.7662\n",
      "Epoch: 66, Loss: 0.47435539960861206, Val Loss: 0.5206094980239868, Val Accuracy: 0.7792\n",
      "Epoch: 67, Loss: 0.47230538725852966, Val Loss: 0.5243998169898987, Val Accuracy: 0.7792\n",
      "Epoch: 68, Loss: 0.4713176488876343, Val Loss: 0.5270042419433594, Val Accuracy: 0.7727\n",
      "Epoch: 69, Loss: 0.4694388806819916, Val Loss: 0.5283498167991638, Val Accuracy: 0.7662\n",
      "Epoch: 70, Loss: 0.4676329791545868, Val Loss: 0.5307530760765076, Val Accuracy: 0.7662\n",
      "Epoch: 71, Loss: 0.4661882519721985, Val Loss: 0.5358985066413879, Val Accuracy: 0.7662\n",
      "Epoch: 72, Loss: 0.46370813250541687, Val Loss: 0.5448671579360962, Val Accuracy: 0.7727\n",
      "Epoch: 73, Loss: 0.4609972834587097, Val Loss: 0.5492255687713623, Val Accuracy: 0.7532\n",
      "Epoch: 74, Loss: 0.45963239669799805, Val Loss: 0.5472729206085205, Val Accuracy: 0.7597\n",
      "Epoch: 75, Loss: 0.45693501830101013, Val Loss: 0.5488235354423523, Val Accuracy: 0.7662\n",
      "Epoch: 76, Loss: 0.4549401104450226, Val Loss: 0.5498416423797607, Val Accuracy: 0.7662\n",
      "Epoch: 77, Loss: 0.45268160104751587, Val Loss: 0.5495412945747375, Val Accuracy: 0.7792\n",
      "Epoch: 78, Loss: 0.45200610160827637, Val Loss: 0.5527766346931458, Val Accuracy: 0.7727\n",
      "Epoch: 79, Loss: 0.449423611164093, Val Loss: 0.5429091453552246, Val Accuracy: 0.7792\n",
      "Epoch: 80, Loss: 0.44761472940444946, Val Loss: 0.5455620884895325, Val Accuracy: 0.7727\n",
      "Epoch: 81, Loss: 0.4453560411930084, Val Loss: 0.5455996990203857, Val Accuracy: 0.7792\n",
      "Epoch: 82, Loss: 0.4432868957519531, Val Loss: 0.548401415348053, Val Accuracy: 0.7727\n",
      "Epoch: 83, Loss: 0.4428448975086212, Val Loss: 0.5587267279624939, Val Accuracy: 0.7662\n",
      "Epoch: 84, Loss: 0.4408728778362274, Val Loss: 0.5581791996955872, Val Accuracy: 0.7597\n",
      "Epoch: 85, Loss: 0.44020432233810425, Val Loss: 0.5536720156669617, Val Accuracy: 0.7597\n",
      "Epoch: 86, Loss: 0.4389132559299469, Val Loss: 0.557940661907196, Val Accuracy: 0.7662\n",
      "Epoch: 87, Loss: 0.4371820092201233, Val Loss: 0.5573614239692688, Val Accuracy: 0.7597\n",
      "Epoch: 88, Loss: 0.4360857903957367, Val Loss: 0.5595750212669373, Val Accuracy: 0.7662\n",
      "Epoch: 89, Loss: 0.43589460849761963, Val Loss: 0.5631014108657837, Val Accuracy: 0.7662\n",
      "Epoch: 90, Loss: 0.4337589144706726, Val Loss: 0.5626356601715088, Val Accuracy: 0.7662\n",
      "Epoch: 91, Loss: 0.43410593271255493, Val Loss: 0.56290203332901, Val Accuracy: 0.7662\n",
      "Epoch: 92, Loss: 0.43300625681877136, Val Loss: 0.565546452999115, Val Accuracy: 0.7662\n",
      "Epoch: 93, Loss: 0.4326511323451996, Val Loss: 0.5657106041908264, Val Accuracy: 0.7597\n",
      "Epoch: 94, Loss: 0.43201538920402527, Val Loss: 0.5697464942932129, Val Accuracy: 0.7532\n",
      "Epoch: 95, Loss: 0.43154096603393555, Val Loss: 0.5699068307876587, Val Accuracy: 0.7532\n",
      "Epoch: 96, Loss: 0.43045157194137573, Val Loss: 0.569342851638794, Val Accuracy: 0.7532\n",
      "Epoch: 97, Loss: 0.43033576011657715, Val Loss: 0.5657938718795776, Val Accuracy: 0.7532\n",
      "Epoch: 98, Loss: 0.430314302444458, Val Loss: 0.5642881393432617, Val Accuracy: 0.7532\n",
      "Epoch: 99, Loss: 0.4290238618850708, Val Loss: 0.5661554336547852, Val Accuracy: 0.7532\n",
      "Epoch: 100, Loss: 0.42868390679359436, Val Loss: 0.5692042708396912, Val Accuracy: 0.7532\n",
      "Epoch: 101, Loss: 0.4284440279006958, Val Loss: 0.5708175301551819, Val Accuracy: 0.7532\n",
      "Epoch: 102, Loss: 0.42824432253837585, Val Loss: 0.5673150420188904, Val Accuracy: 0.7532\n",
      "Epoch: 103, Loss: 0.42799174785614014, Val Loss: 0.5694522857666016, Val Accuracy: 0.7597\n",
      "Epoch: 104, Loss: 0.42740294337272644, Val Loss: 0.5643609762191772, Val Accuracy: 0.7597\n",
      "Epoch: 105, Loss: 0.42642098665237427, Val Loss: 0.5685376524925232, Val Accuracy: 0.7532\n",
      "Epoch: 106, Loss: 0.4268344044685364, Val Loss: 0.5743303298950195, Val Accuracy: 0.7532\n",
      "Epoch: 107, Loss: 0.4271540939807892, Val Loss: 0.5689316987991333, Val Accuracy: 0.7597\n",
      "Epoch: 108, Loss: 0.4261550009250641, Val Loss: 0.5752849578857422, Val Accuracy: 0.7532\n",
      "Epoch: 109, Loss: 0.4263240694999695, Val Loss: 0.5746079683303833, Val Accuracy: 0.7532\n",
      "Epoch: 110, Loss: 0.42715802788734436, Val Loss: 0.569420576095581, Val Accuracy: 0.7532\n",
      "Epoch: 111, Loss: 0.4268367290496826, Val Loss: 0.5780885815620422, Val Accuracy: 0.7468\n",
      "Epoch: 112, Loss: 0.42521464824676514, Val Loss: 0.5695059895515442, Val Accuracy: 0.7597\n",
      "Epoch: 113, Loss: 0.42356061935424805, Val Loss: 0.5759245753288269, Val Accuracy: 0.7532\n",
      "Epoch: 114, Loss: 0.4231198728084564, Val Loss: 0.5783069729804993, Val Accuracy: 0.7468\n",
      "Epoch: 115, Loss: 0.4227006733417511, Val Loss: 0.5790280699729919, Val Accuracy: 0.7468\n",
      "Epoch: 116, Loss: 0.42195287346839905, Val Loss: 0.5799256563186646, Val Accuracy: 0.7468\n",
      "Epoch: 117, Loss: 0.42184746265411377, Val Loss: 0.579092264175415, Val Accuracy: 0.7468\n",
      "Epoch: 118, Loss: 0.42139118909835815, Val Loss: 0.5830989480018616, Val Accuracy: 0.7468\n",
      "Epoch: 119, Loss: 0.4218556582927704, Val Loss: 0.5881471633911133, Val Accuracy: 0.7468\n",
      "Epoch: 120, Loss: 0.42141932249069214, Val Loss: 0.5826915502548218, Val Accuracy: 0.7468\n",
      "Epoch: 121, Loss: 0.42254722118377686, Val Loss: 0.5797998309135437, Val Accuracy: 0.7532\n",
      "Epoch: 122, Loss: 0.4210527241230011, Val Loss: 0.5782949924468994, Val Accuracy: 0.7597\n",
      "Epoch: 123, Loss: 0.41971316933631897, Val Loss: 0.5777996778488159, Val Accuracy: 0.7597\n",
      "Epoch: 124, Loss: 0.4202786982059479, Val Loss: 0.577707052230835, Val Accuracy: 0.7532\n",
      "Epoch: 125, Loss: 0.41930750012397766, Val Loss: 0.5767694115638733, Val Accuracy: 0.7597\n",
      "Epoch: 126, Loss: 0.42014995217323303, Val Loss: 0.5745788812637329, Val Accuracy: 0.7597\n",
      "Epoch: 127, Loss: 0.4191814959049225, Val Loss: 0.5752956867218018, Val Accuracy: 0.7532\n",
      "Epoch: 128, Loss: 0.41910725831985474, Val Loss: 0.5757938027381897, Val Accuracy: 0.7532\n",
      "Epoch: 129, Loss: 0.42015135288238525, Val Loss: 0.5824745893478394, Val Accuracy: 0.7468\n",
      "Epoch: 130, Loss: 0.4183531403541565, Val Loss: 0.5777511596679688, Val Accuracy: 0.7532\n",
      "Epoch: 131, Loss: 0.4190466105937958, Val Loss: 0.5786210298538208, Val Accuracy: 0.7597\n",
      "Epoch: 132, Loss: 0.4181952476501465, Val Loss: 0.5755634307861328, Val Accuracy: 0.7597\n",
      "Epoch: 133, Loss: 0.42024320363998413, Val Loss: 0.5756522417068481, Val Accuracy: 0.7597\n",
      "Epoch: 134, Loss: 0.4184551537036896, Val Loss: 0.5804926156997681, Val Accuracy: 0.7597\n",
      "Epoch: 135, Loss: 0.4184573292732239, Val Loss: 0.5848064422607422, Val Accuracy: 0.7532\n",
      "Epoch: 136, Loss: 0.4166305363178253, Val Loss: 0.5830650925636292, Val Accuracy: 0.7468\n",
      "Epoch: 137, Loss: 0.41522452235221863, Val Loss: 0.5698513388633728, Val Accuracy: 0.7532\n",
      "Epoch: 138, Loss: 0.41520869731903076, Val Loss: 0.5595412850379944, Val Accuracy: 0.7662\n",
      "Epoch: 139, Loss: 0.41726401448249817, Val Loss: 0.5561718344688416, Val Accuracy: 0.7662\n",
      "Epoch: 140, Loss: 0.41184815764427185, Val Loss: 0.5478241443634033, Val Accuracy: 0.7792\n",
      "Epoch: 141, Loss: 0.41387784481048584, Val Loss: 0.5416189432144165, Val Accuracy: 0.7792\n",
      "Epoch: 142, Loss: 0.41616183519363403, Val Loss: 0.5450282096862793, Val Accuracy: 0.7792\n",
      "Epoch: 143, Loss: 0.4204925000667572, Val Loss: 0.5571102499961853, Val Accuracy: 0.7727\n",
      "Epoch: 144, Loss: 0.4160572290420532, Val Loss: 0.5536312460899353, Val Accuracy: 0.7727\n",
      "Epoch: 145, Loss: 0.42626357078552246, Val Loss: 0.5593428611755371, Val Accuracy: 0.7727\n",
      "Epoch: 146, Loss: 0.41894054412841797, Val Loss: 0.5614845752716064, Val Accuracy: 0.7727\n",
      "Epoch: 147, Loss: 0.4211243987083435, Val Loss: 0.5659658908843994, Val Accuracy: 0.7727\n",
      "Epoch: 148, Loss: 0.4199512004852295, Val Loss: 0.5587401390075684, Val Accuracy: 0.7597\n",
      "Epoch: 149, Loss: 0.4194360673427582, Val Loss: 0.5499848127365112, Val Accuracy: 0.7857\n",
      "Epoch: 150, Loss: 0.41827666759490967, Val Loss: 0.5595060586929321, Val Accuracy: 0.7727\n",
      "Epoch: 151, Loss: 0.41844630241394043, Val Loss: 0.5523616075515747, Val Accuracy: 0.7792\n",
      "Epoch: 152, Loss: 0.41785523295402527, Val Loss: 0.5361681580543518, Val Accuracy: 0.7922\n",
      "Epoch: 153, Loss: 0.41515013575553894, Val Loss: 0.5368461608886719, Val Accuracy: 0.7922\n",
      "Epoch: 154, Loss: 0.4132440388202667, Val Loss: 0.55943363904953, Val Accuracy: 0.7727\n",
      "Epoch: 155, Loss: 0.41662663221359253, Val Loss: 0.5787349343299866, Val Accuracy: 0.7662\n",
      "Epoch: 156, Loss: 0.4142910838127136, Val Loss: 0.5805383324623108, Val Accuracy: 0.7662\n",
      "Epoch: 157, Loss: 0.41599345207214355, Val Loss: 0.5651464462280273, Val Accuracy: 0.7727\n",
      "Epoch: 158, Loss: 0.41409832239151, Val Loss: 0.5612086057662964, Val Accuracy: 0.7727\n",
      "Epoch: 159, Loss: 0.414152055978775, Val Loss: 0.5643390417098999, Val Accuracy: 0.7662\n",
      "Epoch: 160, Loss: 0.41401126980781555, Val Loss: 0.5674135088920593, Val Accuracy: 0.7662\n",
      "Epoch: 161, Loss: 0.4123748242855072, Val Loss: 0.5679585337638855, Val Accuracy: 0.7662\n",
      "Epoch: 162, Loss: 0.4127141833305359, Val Loss: 0.5645278096199036, Val Accuracy: 0.7662\n",
      "Epoch: 163, Loss: 0.41220805048942566, Val Loss: 0.5641099810600281, Val Accuracy: 0.7727\n",
      "Epoch: 164, Loss: 0.4119734466075897, Val Loss: 0.5606482625007629, Val Accuracy: 0.7727\n",
      "Epoch: 165, Loss: 0.41258665919303894, Val Loss: 0.5562029480934143, Val Accuracy: 0.7727\n",
      "Epoch: 166, Loss: 0.41167667508125305, Val Loss: 0.5531723499298096, Val Accuracy: 0.7792\n",
      "Epoch: 167, Loss: 0.4118855595588684, Val Loss: 0.5551731586456299, Val Accuracy: 0.7727\n",
      "Epoch: 168, Loss: 0.41127297282218933, Val Loss: 0.5600223541259766, Val Accuracy: 0.7727\n",
      "Epoch: 169, Loss: 0.4111497402191162, Val Loss: 0.5668092370033264, Val Accuracy: 0.7727\n",
      "Epoch: 170, Loss: 0.4110645353794098, Val Loss: 0.5692017078399658, Val Accuracy: 0.7792\n",
      "Epoch: 171, Loss: 0.4108823239803314, Val Loss: 0.5639853477478027, Val Accuracy: 0.7662\n",
      "Epoch: 172, Loss: 0.4109257161617279, Val Loss: 0.5605247020721436, Val Accuracy: 0.7727\n",
      "Epoch: 173, Loss: 0.4106299877166748, Val Loss: 0.5610036849975586, Val Accuracy: 0.7727\n",
      "Epoch: 174, Loss: 0.4106595814228058, Val Loss: 0.5630444884300232, Val Accuracy: 0.7727\n",
      "Epoch: 175, Loss: 0.4105423390865326, Val Loss: 0.5632992386817932, Val Accuracy: 0.7662\n",
      "Epoch: 176, Loss: 0.410598486661911, Val Loss: 0.5616002678871155, Val Accuracy: 0.7662\n",
      "Epoch: 177, Loss: 0.4103274643421173, Val Loss: 0.5632848739624023, Val Accuracy: 0.7662\n",
      "Epoch: 178, Loss: 0.41064780950546265, Val Loss: 0.5649507641792297, Val Accuracy: 0.7727\n",
      "Epoch: 179, Loss: 0.4101922810077667, Val Loss: 0.567256510257721, Val Accuracy: 0.7727\n",
      "Epoch: 180, Loss: 0.4104732871055603, Val Loss: 0.5681200623512268, Val Accuracy: 0.7662\n",
      "Epoch: 181, Loss: 0.4108658730983734, Val Loss: 0.5685329437255859, Val Accuracy: 0.7727\n",
      "Epoch: 182, Loss: 0.4101417064666748, Val Loss: 0.5663886070251465, Val Accuracy: 0.7727\n",
      "Epoch: 183, Loss: 0.4104490876197815, Val Loss: 0.5601631999015808, Val Accuracy: 0.7662\n",
      "Epoch: 184, Loss: 0.4100891053676605, Val Loss: 0.555888831615448, Val Accuracy: 0.7727\n",
      "Epoch: 185, Loss: 0.41012686491012573, Val Loss: 0.5518537163734436, Val Accuracy: 0.7662\n",
      "Epoch: 186, Loss: 0.41026952862739563, Val Loss: 0.5496788620948792, Val Accuracy: 0.7597\n",
      "Epoch: 187, Loss: 0.4100255072116852, Val Loss: 0.5492082834243774, Val Accuracy: 0.7597\n",
      "Epoch: 188, Loss: 0.4099333584308624, Val Loss: 0.5502708554267883, Val Accuracy: 0.7597\n",
      "Epoch: 189, Loss: 0.4097583293914795, Val Loss: 0.552743136882782, Val Accuracy: 0.7597\n",
      "Epoch: 190, Loss: 0.409859299659729, Val Loss: 0.5542113780975342, Val Accuracy: 0.7727\n",
      "Epoch: 191, Loss: 0.40980666875839233, Val Loss: 0.5550115704536438, Val Accuracy: 0.7597\n",
      "Epoch: 192, Loss: 0.40985575318336487, Val Loss: 0.5577296614646912, Val Accuracy: 0.7662\n",
      "Epoch: 193, Loss: 0.40979301929473877, Val Loss: 0.5606682300567627, Val Accuracy: 0.7662\n",
      "Epoch: 194, Loss: 0.4096972644329071, Val Loss: 0.5623084306716919, Val Accuracy: 0.7597\n",
      "Epoch: 195, Loss: 0.4095807671546936, Val Loss: 0.5641909837722778, Val Accuracy: 0.7532\n",
      "Epoch: 196, Loss: 0.4096156358718872, Val Loss: 0.5688329339027405, Val Accuracy: 0.7532\n",
      "Epoch: 197, Loss: 0.40955862402915955, Val Loss: 0.5658669471740723, Val Accuracy: 0.7468\n",
      "Epoch: 198, Loss: 0.40959736704826355, Val Loss: 0.5661435723304749, Val Accuracy: 0.7532\n",
      "Epoch: 199, Loss: 0.4096020460128784, Val Loss: 0.5697756409645081, Val Accuracy: 0.7597\n",
      "Epoch: 200, Loss: 0.4095817506313324, Val Loss: 0.5710464715957642, Val Accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    # Training\n",
    "    y_pred = newmodel(X_train_tensor)\n",
    "    loss = loss_function(y_pred, y_train_tensor.view(-1, 1))\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = newmodel(X_test_tensor)\n",
    "        val_loss = loss_function(y_pred_val, y_test_tensor.view(-1, 1))\n",
    "        y_pred_val_binary = (y_pred_val > 0.3).float()\n",
    "        val_accuracy = (y_pred_val_binary == y_test_tensor.view(-1, 1)).sum() / len(y_test_tensor)\n",
    "    \n",
    "    print(f\"Epoch: {i+1}, Loss: {loss}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.param_groups[0]['lr']=loss/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=newmodel(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = (y_test_pred > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djdeb\\AppData\\Local\\Temp\\ipykernel_5776\\3705519333.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred_binary=torch.tensor(y_pred_binary,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_pred_binary=torch.tensor(y_pred_binary,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7467532467532467"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_binary,y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
